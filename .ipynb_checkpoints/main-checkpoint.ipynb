{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6da01c6-af59-44ba-8f89-ddd3dc454702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 21:41:39.045793: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-22 21:41:39.084854: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Base Model: facebook/wav2vec2-conformer-rope-large-960h-ft...\n",
      "Checkpoint found in conformer_lora_best! Attempting to load...\n",
      "Weight Sum BEFORE Load (LM Head): 1.6663\n",
      "Weight Sum AFTER Load (LM Head): 1.6663\n",
      "trainable params: 293,925 || all params: 604,392,101 || trainable%: 0.0486\n",
      "\n",
      "Starting RESUMED Training V3...\n",
      "Resuming with previous best WER baseline: 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|█████████████████████████████████████████| 3000/3000 [13:18<00:00,  3.76it/s, Loss=3.2242, LR=4.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 13.9236 | WER: 1.0000 | CER: 1.0000\n",
      "No improvement over best WER: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25:   5%|█▉                                        | 142/3000 [00:31<10:24,  4.58it/s, Loss=3.1312, LR=4.19e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 344\u001b[0m\n\u001b[1;32m    341\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 344\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 283\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(features, attention_mask\u001b[38;5;241m=\u001b[39mmask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m    281\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_accum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 283\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_accum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    286\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[0;32m/m/scicomp/software/scibuilder-mamba/aalto-ubuntu2204/prod/software/scicomp-python-env/2025.2/b4b5f8e/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/m/scicomp/software/scibuilder-mamba/aalto-ubuntu2204/prod/software/scicomp-python-env/2025.2/b4b5f8e/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/m/scicomp/software/scibuilder-mamba/aalto-ubuntu2204/prod/software/scicomp-python-env/2025.2/b4b5f8e/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer\n",
    "import gc\n",
    "import types\n",
    "from peft import PeftModel\n",
    "import warnings\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Hugging Face Imports\n",
    "from transformers import (\n",
    "    Wav2Vec2ConformerForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from huggingface_hub import login\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"train_csv\": \"geo/train.csv\",\n",
    "    \"val_csv\": \"geo/dev.csv\",\n",
    "    \"hf_token\": None,\n",
    "    \n",
    "    # --- TUNING V3 ---\n",
    "    \"mask_time_prob\": 0.075,     \n",
    "    \"mask_time_length\": 10,\n",
    "    \"mask_feature_prob\": 0.075,\n",
    "    \"mask_feature_length\": 10,\n",
    "    \n",
    "    \"lora_rank\": 64,             \n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    \"batch_size\": 2,           \n",
    "    \"grad_accum_steps\": 4,     \n",
    "    \"max_audio_len\": 160000,   \n",
    "    \"learning_rate\": 1e-4,       \n",
    "    \"num_epochs\": 25,            \n",
    "    \"base_model\": \"facebook/wav2vec2-conformer-rope-large-960h-ft\",\n",
    "    \"target_modules\": [\"linear_q\", \"linear_k\", \"linear_v\", \"linear_out\", \"intermediate_dense\", \"output_dense\"],\n",
    "    \n",
    "    # Checkpoint Configuration\n",
    "    # Make sure this path points to the folder containing 'adapter_model.bin'\n",
    "    \"checkpoint_path\": \"conformer_lora_best\", \n",
    "    \"previous_best_wer\": 0.38\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Auth ---\n",
    "auth_token_arg = False \n",
    "if CONFIG[\"hf_token\"]:\n",
    "    login(token=CONFIG[\"hf_token\"])\n",
    "    auth_token_arg = True\n",
    "\n",
    "# --- Vocabulary Builder ---\n",
    "class Vocabulary:\n",
    "    def __init__(self, csv_paths):\n",
    "        self.vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3, \"|\": 4}\n",
    "        self.idx = 5\n",
    "        self._build_vocab(csv_paths)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def _build_vocab(self, paths):\n",
    "        chars = set()\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path): continue\n",
    "            df = pd.read_csv(path)\n",
    "            all_text = \"\".join(df['transcript'].fillna(\"\").astype(str).tolist())\n",
    "            chars.update(list(all_text))\n",
    "        for c in sorted(chars):\n",
    "            if c not in self.vocab:\n",
    "                self.vocab[c] = self.idx\n",
    "                self.idx += 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = text.replace(\" \", \"|\")\n",
    "        return [self.vocab.get(c, self.vocab[\"<unk>\"]) for c in text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        res = []\n",
    "        for t in tokens:\n",
    "            if t == 0: continue \n",
    "            char = self.inv_vocab.get(t, \"\")\n",
    "            res.append(char)\n",
    "        return \"\".join(res).replace(\"|\", \" \").replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "# --- Dataset ---\n",
    "class EsperantoDataset(Dataset):\n",
    "    def __init__(self, csv_path, vocab, processor, max_len=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.vocab = vocab\n",
    "        self.processor = processor \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['file']\n",
    "        transcript = str(self.df.iloc[idx]['transcript']) if pd.notna(self.df.iloc[idx]['transcript']) else \"\"\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            waveform = waveform.squeeze()\n",
    "\n",
    "            if self.max_len and waveform.size(0) > self.max_len:\n",
    "                waveform = waveform[:self.max_len]\n",
    "\n",
    "            features = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "            labels = torch.tensor(self.vocab.encode(transcript), dtype=torch.long)\n",
    "            return features, labels\n",
    "        except Exception:\n",
    "            return torch.zeros(16000), torch.tensor([0], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[1].sum() != 0]\n",
    "    if not batch: return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "    \n",
    "    features, labels = zip(*batch)\n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0.0)\n",
    "    attention_mask = (features_padded != 0).long() \n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return features_padded, attention_mask, labels_padded\n",
    "\n",
    "# --- Model Setup ---\n",
    "def get_lora_conformer(vocab_size):\n",
    "    print(f\"Loading Base Model: {CONFIG['base_model']}...\")\n",
    "    \n",
    "    model = Wav2Vec2ConformerForCTC.from_pretrained(\n",
    "        CONFIG[\"base_model\"], \n",
    "        ctc_loss_reduction=\"mean\", \n",
    "        pad_token_id=0,\n",
    "        vocab_size=vocab_size,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        token=auth_token_arg,\n",
    "        mask_time_prob=CONFIG[\"mask_time_prob\"],\n",
    "        mask_time_length=CONFIG[\"mask_time_length\"],\n",
    "        mask_feature_prob=CONFIG[\"mask_feature_prob\"],\n",
    "        mask_feature_length=CONFIG[\"mask_feature_length\"],\n",
    "    )\n",
    "    \n",
    "    # --- Monkey Patching ---\n",
    "    def enable_input_require_grads(self):\n",
    "        def make_inputs_require_grads(module, input, output):\n",
    "            if isinstance(output, tuple): output[0].requires_grad_(True) \n",
    "            else: output.requires_grad_(True)\n",
    "        self._require_grads_hook = self.wav2vec2_conformer.feature_projection.register_forward_hook(make_inputs_require_grads)\n",
    "\n",
    "    model.enable_input_require_grads = types.MethodType(enable_input_require_grads, model)\n",
    "    def get_input_embeddings(self): return self.wav2vec2_conformer.feature_projection\n",
    "    model.get_input_embeddings = types.MethodType(get_input_embeddings, model)\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Freeze Logic - Start by freezing everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    checkpoint_exists = os.path.exists(CONFIG[\"checkpoint_path\"])\n",
    "\n",
    "    if checkpoint_exists:\n",
    "        print(f\"Checkpoint found! Loading LoRA weights from {CONFIG['checkpoint_path']}...\")\n",
    "        # Load adapters and ensure they are trainable\n",
    "        model = PeftModel.from_pretrained(model, CONFIG[\"checkpoint_path\"], is_trainable=True)\n",
    "        \n",
    "        # FORCE TRAINABLE: Sometimes is_trainable=True isn't enough if base is frozen\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"lora\" in name:\n",
    "                param.requires_grad = True\n",
    "    else:\n",
    "        print(\"Initializing new LoRA adapters...\")\n",
    "        peft_config = LoraConfig(\n",
    "            inference_mode=False,\n",
    "            r=CONFIG[\"lora_rank\"],\n",
    "            lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "            lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "            target_modules=CONFIG[\"target_modules\"]\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Re-apply Unfreezing Logic (Must be done AFTER loading PEFT)\n",
    "    \n",
    "    # 1. Unfreeze LayerNorms\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer_norm\" in name:\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    # 2. Unfreeze Feature Extractor LayerNorms\n",
    "    # Accessing the base model inside PeftModel wrapper\n",
    "    base = model.base_model.model if hasattr(model.base_model, \"model\") else model.base_model\n",
    "    for name, param in base.wav2vec2_conformer.feature_extractor.named_parameters():\n",
    "        if \"layer_norm\" in name or \"ln\" in name: \n",
    "             param.requires_grad = True\n",
    "\n",
    "    # 3. Unfreeze LM Head\n",
    "    if hasattr(model, \"lm_head\"):\n",
    "         for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "    elif hasattr(model.base_model, \"lm_head\"): \n",
    "         for param in model.base_model.lm_head.parameters(): param.requires_grad = True\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model, checkpoint_exists\n",
    "\n",
    "# --- Main ---\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    vocab = Vocabulary([CONFIG[\"train_csv\"], CONFIG[\"val_csv\"]])\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG[\"base_model\"], token=auth_token_arg)\n",
    "    \n",
    "    train_dataset = EsperantoDataset(CONFIG[\"train_csv\"], vocab, processor, max_len=CONFIG[\"max_audio_len\"])\n",
    "    val_dataset = EsperantoDataset(CONFIG[\"val_csv\"], vocab, processor) \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn, num_workers=1)\n",
    "\n",
    "    model, checkpoint_loaded = get_lora_conformer(len(vocab.vocab))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Adjust steps for remaining epochs if resuming? \n",
    "    # Simpler to just treat it as a new run with 'num_epochs'\n",
    "    num_training_steps = len(train_loader) * CONFIG[\"num_epochs\"] // CONFIG[\"grad_accum_steps\"]\n",
    "    num_warmup_steps = int(0.1 * num_training_steps) \n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    status = \"RESUMED\" if checkpoint_loaded else \"NEW\"\n",
    "    print(f\"\\nStarting {status} Training V3...\")\n",
    "    \n",
    "    if checkpoint_loaded:\n",
    "        best_wer = CONFIG.get(\"previous_best_wer\", float('inf'))\n",
    "        print(f\"Resuming with previous best WER baseline: {best_wer}\")\n",
    "    else:\n",
    "        best_wer = float('inf')\n",
    "\n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        for step, (features, mask, labels) in enumerate(pbar):\n",
    "            if features.size(0) == 0: continue \n",
    "\n",
    "            features = features.to(device)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(features, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss / CONFIG[\"grad_accum_steps\"]\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % CONFIG[\"grad_accum_steps\"] == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * CONFIG[\"grad_accum_steps\"]\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item()*CONFIG['grad_accum_steps']:.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        refs, preds = [], []\n",
    "        \n",
    "        print(f\"Evaluating Epoch {epoch+1}...\")\n",
    "        with torch.no_grad():\n",
    "            for features, mask, labels in val_loader:\n",
    "                if features.size(0) == 0: continue\n",
    "                features = features.to(device)\n",
    "                mask = mask.to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(features, attention_mask=mask)\n",
    "                \n",
    "                logits = outputs.logits.float()\n",
    "                pred_ids = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                for i in range(len(labels)):\n",
    "                    label_idx = labels[i][labels[i] != -100].cpu().tolist()\n",
    "                    pred_idx = pred_ids[i].cpu().tolist()\n",
    "                    \n",
    "                    pred_str_raw = []\n",
    "                    prev_token = -1\n",
    "                    for token in pred_idx:\n",
    "                        if token != prev_token and token != 0: \n",
    "                            pred_str_raw.append(token)\n",
    "                        prev_token = token\n",
    "                    \n",
    "                    refs.append(vocab.decode(label_idx))\n",
    "                    preds.append(vocab.decode(pred_str_raw))\n",
    "\n",
    "        epoch_wer = wer(refs, preds) if len(refs) > 0 else 1.0\n",
    "        epoch_cer = cer(refs, preds) if len(refs) > 0 else 1.0\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | WER: {epoch_wer:.4f} | CER: {epoch_cer:.4f}\")\n",
    "        \n",
    "        if epoch_wer < best_wer:\n",
    "            best_wer = epoch_wer\n",
    "            model.save_pretrained(CONFIG[\"checkpoint_path\"])\n",
    "            print(f\"New Best Model Saved! (WER: {best_wer:.4f})\")\n",
    "        else:\n",
    "            print(f\"No improvement over best WER: {best_wer:.4f}\")\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
