{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1af896-e80c-4746-a864-ecbaab313713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16200534-c0e3-4eb7-987d-c12cadd08c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Paths\n",
    "CSV_PATH = 'geo/train.csv'\n",
    "DATA_DIR = 'geo/clips'\n",
    "SAMPLE_RATE = 16000  # Assuming 16kHz; adjust if needed\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160  # 10ms hop\n",
    "WIN_LENGTH = 400  # 25ms window\n",
    "\n",
    "# Vocabulary: Esperanto characters + specials (adjust as needed for gibberish)\n",
    "CHAR_MAP = {c: i+3 for i, c in enumerate('abcĉdefgĝhĥijĵklmnoprsŝtuŭvz ')}  # Lowercase, add accents and space\n",
    "CHAR_MAP['<PAD>'] = 0\n",
    "CHAR_MAP['<SOS>'] = 1\n",
    "CHAR_MAP['<EOS>'] = 2\n",
    "VOCAB_SIZE = len(CHAR_MAP)\n",
    "INV_CHAR_MAP = {v: k for k, v in CHAR_MAP.items()}\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)  # Assumes columns: 'file', 'transcript'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['file']\n",
    "        transcript = self.df.iloc[idx]['transcript'].strip().lower()\n",
    "\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            waveform = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(waveform)\n",
    "\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, n_fft=WIN_LENGTH\n",
    "        )\n",
    "        mel_spec = torch.log(mel_transform(waveform) + 1e-9)  # Log-Mel\n",
    "        mel_spec = mel_spec.squeeze(0).transpose(0, 1)  # (seq_len, n_mels)\n",
    "\n",
    "        target = [CHAR_MAP['<SOS>']] + [CHAR_MAP[c] for c in transcript if c in CHAR_MAP] + [CHAR_MAP['<EOS>']]\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return mel_spec, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    mels, targets = zip(*batch)\n",
    "    mel_lens = torch.tensor([len(m) for m in mels])\n",
    "    target_lens = torch.tensor([len(t) for t in targets])\n",
    "    mels_padded = pad_sequence(mels, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=CHAR_MAP['<PAD>'])\n",
    "    return mels_padded, targets_padded, mel_lens, target_lens\n",
    "\n",
    "# Model components\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=256):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[..., None] * emb[None, :]  # Broadcasting fix\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=80, hidden_dim=256, num_layers=6, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.conv_sub = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=1024, dropout=0.1, batch_first=True)\n",
    "        self.transformer_enc = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, 80)\n",
    "        x = x.transpose(1, 2)  # For conv1d: (batch, 80, seq_len)\n",
    "        x = self.conv_sub(x)  # (batch, 256, seq_len//4)\n",
    "        x = x.transpose(1, 2)  # (batch, seq_len//4, 256)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(0, seq_len, device=x.device).unsqueeze(0).repeat(x.size(0), 1)\n",
    "        x = x + self.pos_emb(pos)\n",
    "        mask = torch.arange(seq_len, device=x.device)[None, :] >= (lengths // 4)[:, None]\n",
    "        x = self.transformer_enc(x, src_key_padding_mask=mask)\n",
    "        return x, mask\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "\n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        q = self.query_proj(query)\n",
    "        k = self.key_proj(keys)\n",
    "        v = self.value_proj(values)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1), -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        return context\n",
    "\n",
    "class RecurrentDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, hidden_dim=512, enc_dim=256, vocab_size=VOCAB_SIZE, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=CHAR_MAP['<PAD>'])\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.1)\n",
    "        self.memory_proj = nn.Linear(enc_dim, hidden_dim)  # New: Project encoder dim to decoder hidden_dim\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)  # Concat rnn_out + context\n",
    "        self.pos_emb = SinusoidalPosEmb(embed_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_lengths, memory_mask):\n",
    "        # tgt: (batch, tgt_len)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        pos = torch.arange(0, tgt_seq_len, device=tgt.device).unsqueeze(0).repeat(tgt.size(0), 1)\n",
    "        tgt_emb = tgt_emb + self.pos_emb(pos)\n",
    "        packed_tgt = pack_padded_sequence(tgt_emb, tgt_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_tgt)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        # Project memory to match decoder dim\n",
    "        memory_proj = self.memory_proj(memory)\n",
    "        \n",
    "        # Attention: query=rnn_out, key=value=memory_proj\n",
    "        context = self.attention(rnn_out, memory_proj, memory_proj, memory_mask)\n",
    "        \n",
    "        combined = torch.cat((rnn_out, context), dim=-1)\n",
    "        logits = self.fc(combined)\n",
    "        return logits\n",
    "\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = AudioEncoder()\n",
    "        self.decoder = RecurrentDecoder()  # Defaults to enc_dim=256\n",
    "\n",
    "    def forward(self, src, tgt, src_lengths, tgt_lengths):\n",
    "        enc_out, enc_mask = self.encoder(src, src_lengths)\n",
    "        logits = self.decoder(tgt, enc_out, tgt_lengths, enc_mask)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdca84ad-90e7-4545-85b9-1aedbe468cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASRModel(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv_sub): Sequential(\n",
      "      (0): Conv1d(80, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (transformer_enc): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pos_emb): SinusoidalPosEmb()\n",
      "  )\n",
      "  (decoder): RecurrentDecoder(\n",
      "    (embedding): Embedding(32, 256, padding_idx=0)\n",
      "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (memory_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (attention): Attention(\n",
      "      (query_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (key_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (value_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (fc): Linear(in_features=1024, out_features=32, bias=True)\n",
      "    (pos_emb): SinusoidalPosEmb()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "\n",
    "dataset = AudioDataset(CSV_PATH)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = ASRModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CHAR_MAP['<PAD>'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7188762-6daf-4bf0-b67a-a1d6fa9ae766",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50  # Adjust based on your dataset size and convergence\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for mels, targets, mel_lens, target_lens in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Teacher forcing: input_tgt = targets[:, :-1], label = targets[:, 1:]\n",
    "        input_tgt = targets[:, :-1]\n",
    "        label = targets[:, 1:]\n",
    "        input_tgt_lens = target_lens - 1\n",
    "\n",
    "        logits = model(mels, input_tgt, mel_lens, input_tgt_lens)\n",
    "        # logits: (batch, tgt_len, vocab), label: (batch, tgt_len)\n",
    "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), label.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'asr_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
