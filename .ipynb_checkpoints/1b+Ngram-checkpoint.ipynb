{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c13bb-407e-428f-b486-df358b94c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 01:25:36.410759: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-01 01:25:36.449455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing Processor...\n",
      "Loaded 6000 valid samples from geo/train.csv\n",
      "Loaded 1000 valid samples from geo/dev.csv\n",
      "Loading Base Model: facebook/wav2vec2-xls-r-1b...\n",
      "\n",
      "[INFO] Found existing 1B checkpoint. Resuming...\n",
      "trainable params: 71,082,789 || all params: 1,033,627,594 || trainable%: 6.8770\n",
      "\n",
      "Starting 1B Model Training...\n",
      "Baseline restored -> WER: 0.1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   2%|â–‹                                          | 97/6000 [00:19<20:14,  4.86it/s, Loss=0.2617, LR=1.60e-07]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer\n",
    "import gc\n",
    "import types\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import shutil\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Gain\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face Imports\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from huggingface_hub import login\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "#Define the augmentation pipeline\n",
    "augmentations = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),\n",
    "    TimeStretch(min_rate=0.85, max_rate=1.15, p=0.3),\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.3),\n",
    "])\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.getLogger(\"torchaudio\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- Configuration for 1B Model on RTX 3080 ---\n",
    "CONFIG = {\n",
    "    \"train_csv\": \"geo/train.csv\",\n",
    "    \"val_csv\": \"geo/dev.csv\",\n",
    "    \"hf_token\": None,  \n",
    "    \n",
    "    # --- 1B MODEL SETTINGS ---\n",
    "    \"base_model\": \"facebook/wav2vec2-xls-r-1b\", # <--- The 1B Model\n",
    "    \"checkpoint_path\": \"xlsr_1b_Ngram\", #\n",
    "    \n",
    "    \"batch_size\": 1,             # Must be 1 to fit in memory\n",
    "    \"grad_accum_steps\": 16,      # Increase this to simulate batch size 16\n",
    "    \"max_audio_len\": 160000,     # Reduced slightly (8 seconds) to prevent OOM\n",
    "    \n",
    "    \"lora_rank\": 64,             \n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    \"learning_rate\": 5e-5,       \n",
    "    \"num_epochs\": 50,            \n",
    "    \n",
    "    \"mask_time_prob\": 0.15,      \n",
    "    \"mask_time_length\": 10,\n",
    "    \"mask_feature_prob\": 0.10,   \n",
    "    \"mask_feature_length\": 64,\n",
    "    \n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"intermediate_dense\", \"output_dense\"],\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if CONFIG[\"hf_token\"]:\n",
    "    login(token=CONFIG[\"hf_token\"])\n",
    "\n",
    "# --- Vocabulary Builder ---\n",
    "class Vocabulary:\n",
    "    def __init__(self, csv_paths):\n",
    "        self.vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3, \"|\": 4}\n",
    "        self.idx = 5\n",
    "        self._build_vocab(csv_paths)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def _build_vocab(self, paths):\n",
    "        chars = set()\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path): continue\n",
    "            df = pd.read_csv(path)\n",
    "            all_text = \"\".join(df['transcript'].fillna(\"\").astype(str).tolist())\n",
    "            chars.update(list(all_text))\n",
    "        \n",
    "        for c in sorted(chars):\n",
    "            if c not in self.vocab:\n",
    "                self.vocab[c] = self.idx\n",
    "                self.idx += 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = str(text).replace(\" \", \"|\")\n",
    "        return [self.vocab.get(c, self.vocab[\"<unk>\"]) for c in text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        res = []\n",
    "        for t in tokens:\n",
    "            if t == 0: continue \n",
    "            char = self.inv_vocab.get(t, \"\")\n",
    "            res.append(char)\n",
    "        return \"\".join(res).replace(\"|\", \" \").replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "# --- Dataset ---\n",
    "class EsperantoDataset(Dataset):\n",
    "    def __init__(self, csv_path, vocab, processor, max_len=None, augment=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df = self.df[self.df['file'].apply(os.path.exists)].reset_index(drop=True)\n",
    "        print(f\"Loaded {len(self.df)} valid samples from {csv_path}\")\n",
    "        self.vocab = vocab\n",
    "        self.processor = processor \n",
    "        self.max_len = max_len\n",
    "        self.augment = augment  # <--- Flag to turn on/off\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['file']\n",
    "        transcript = self.df.iloc[idx]['transcript']\n",
    "        if pd.isna(transcript): transcript = \"\"\n",
    "        transcript = str(transcript)\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "            \n",
    "            # Mix to mono\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            waveform = waveform.squeeze()\n",
    "\n",
    "            # --- AUGMENTATION BLOCK START ---\n",
    "            if self.augment:\n",
    "                # 1. Apply Audiomentations (TimeStretch, PitchShift, Noise)\n",
    "                wav_numpy = waveform.numpy()\n",
    "                # Ensure it's not empty before augmenting\n",
    "                if len(wav_numpy) > 0:\n",
    "                    try:\n",
    "                        wav_numpy = augmentations(samples=wav_numpy, sample_rate=16000)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Augmentation warning: {e}\")\n",
    "\n",
    "                # 2. Apply Manual Gain (Replaces the broken Gain class)\n",
    "                # Randomly scale volume by +/- 6dB (factor 0.5x to 2.0x)\n",
    "                if np.random.random() < 0.3:\n",
    "                    gain_factor = 10 ** (np.random.uniform(-6, 6) / 20)\n",
    "                    wav_numpy = wav_numpy * gain_factor\n",
    "\n",
    "                # Convert back to tensor\n",
    "                waveform = torch.from_numpy(wav_numpy).float()\n",
    "            # --- AUGMENTATION BLOCK END ---\n",
    "\n",
    "            if self.max_len and waveform.size(0) > self.max_len:\n",
    "                waveform = waveform[:self.max_len]\n",
    "\n",
    "            features = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "            labels = torch.tensor(self.vocab.encode(transcript), dtype=torch.long)\n",
    "            return features, labels\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading {audio_path}: {e}\") # Optional: debug print\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch: return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "    features, labels = zip(*batch)\n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0.0)\n",
    "    attention_mask = (features_padded != 0).long() \n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return features_padded, attention_mask, labels_padded\n",
    "\n",
    "# --- Model Loading (1B Configured) ---\n",
    "def get_lora_model(vocab_size):\n",
    "    print(f\"Loading Base Model: {CONFIG['base_model']}...\")\n",
    "    use_auth_token = CONFIG[\"hf_token\"] if CONFIG[\"hf_token\"] else False\n",
    "\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        CONFIG[\"base_model\"], \n",
    "        ctc_loss_reduction=\"mean\", \n",
    "        pad_token_id=0,\n",
    "        vocab_size=vocab_size,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        token=use_auth_token, # <--- Fix applied here\n",
    "        mask_time_prob=CONFIG[\"mask_time_prob\"],\n",
    "        mask_time_length=CONFIG[\"mask_time_length\"],\n",
    "        mask_feature_prob=CONFIG[\"mask_feature_prob\"],\n",
    "        mask_feature_length=CONFIG[\"mask_feature_length\"],\n",
    "    )\n",
    "    \n",
    "    if not hasattr(model, \"enable_input_require_grads\"):\n",
    "        def enable_input_require_grads(self):\n",
    "            def make_inputs_require_grads(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            self.wav2vec2.feature_projection.register_forward_hook(make_inputs_require_grads)\n",
    "        model.enable_input_require_grads = types.MethodType(enable_input_require_grads, model)\n",
    "\n",
    "    # We use a new checkpoint path for 1B\n",
    "    adapter_path = os.path.join(CONFIG[\"checkpoint_path\"], \"adapter_model.safetensors\")\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        print(f\"\\n[INFO] Found existing 1B checkpoint. Resuming...\")\n",
    "        model = PeftModel.from_pretrained(model, CONFIG[\"checkpoint_path\"], is_trainable=True)\n",
    "    else:\n",
    "        print(f\"\\n[INFO] Initializing FRESH 1B LoRA model...\")\n",
    "        modules_to_save = [\"lm_head\", \"layer_norm\"] \n",
    "        peft_config = LoraConfig(\n",
    "            inference_mode=False,\n",
    "            r=CONFIG[\"lora_rank\"],\n",
    "            lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "            lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "            target_modules=CONFIG[\"target_modules\"],\n",
    "            modules_to_save=modules_to_save\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, vocab):\n",
    "    model.eval()\n",
    "    refs, preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, mask, labels in dataloader:\n",
    "            if features.size(0) == 0: continue\n",
    "            features = features.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(features, attention_mask=mask)\n",
    "            \n",
    "            logits = outputs.logits.float().cpu().numpy()\n",
    "            pred_ids = logits.argmax(axis=-1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                label_idx = labels[i][labels[i] != -100].cpu().tolist()\n",
    "                refs.append(vocab.decode(label_idx))\n",
    "                \n",
    "                pred_str_raw = []\n",
    "                prev_token = -1\n",
    "                for token in pred_ids[i]:\n",
    "                    if token != prev_token and token != 0: \n",
    "                        pred_str_raw.append(token)\n",
    "                    prev_token = token\n",
    "                preds.append(vocab.decode(pred_str_raw))\n",
    "\n",
    "    epoch_wer = wer(refs, preds) if len(refs) > 0 else 1.0\n",
    "    epoch_cer = cer(refs, preds) if len(refs) > 0 else 1.0\n",
    "    return epoch_wer, epoch_cer\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    vocab = Vocabulary([CONFIG[\"train_csv\"], CONFIG[\"val_csv\"]])\n",
    "    use_auth_token = CONFIG[\"hf_token\"] if CONFIG[\"hf_token\"] else False\n",
    "\n",
    "    print(\"Initializing Processor...\")\n",
    "    # --- FIX: This is the line that was crashing your notebook ---\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        CONFIG[\"base_model\"], \n",
    "        token=use_auth_token # Added token=False/Token logic\n",
    "    )\n",
    "    \n",
    "    train_dataset = EsperantoDataset(CONFIG[\"train_csv\"], vocab, processor, max_len=CONFIG[\"max_audio_len\"], augment=True)\n",
    "    val_dataset = EsperantoDataset(CONFIG[\"val_csv\"], vocab, processor, augment=False) # Keep validation clean!\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn, num_workers=1)\n",
    "\n",
    "    model = get_lora_model(len(vocab.vocab))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    num_training_steps = len(train_loader) * CONFIG[\"num_epochs\"] // CONFIG[\"grad_accum_steps\"]\n",
    "    num_warmup_steps = int(0.1 * num_training_steps) \n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    print(f\"\\nStarting 1B Model Training...\")\n",
    "    best_wer = float('inf')\n",
    "    \n",
    "    if os.path.exists(os.path.join(CONFIG[\"checkpoint_path\"], \"adapter_model.safetensors\")):\n",
    "        baseline_wer, baseline_cer = evaluate(model, val_loader, vocab)\n",
    "        best_wer = baseline_wer\n",
    "        print(f\"Baseline restored -> WER: {baseline_wer:.4f}\")\n",
    "    \n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        for step, (features, mask, labels) in enumerate(pbar):\n",
    "            if features.size(0) == 0: continue \n",
    "\n",
    "            features = features.to(device)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(features, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss / CONFIG[\"grad_accum_steps\"]\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % CONFIG[\"grad_accum_steps\"] == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # OPTIONAL: Explicit memory cleanup for 1B model\n",
    "                # del loss, outputs\n",
    "                # torch.cuda.empty_cache()\n",
    "\n",
    "            total_loss += loss.item() * CONFIG[\"grad_accum_steps\"]\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item()*CONFIG['grad_accum_steps']:.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Evaluating Epoch {epoch+1}...\")\n",
    "        epoch_wer, epoch_cer = evaluate(model, val_loader, vocab)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_train_loss:.4f} | WER: {epoch_wer:.4f} | CER: {epoch_cer:.4f}\")\n",
    "        \n",
    "        if epoch_wer < best_wer:\n",
    "            best_wer = epoch_wer\n",
    "            model.save_pretrained(CONFIG[\"checkpoint_path\"])\n",
    "            print(f\"New Best Model Saved! (WER: {best_wer:.4f})\")\n",
    "            \n",
    "            with open(\"vocab_1bfr.json\", \"w\") as f:\n",
    "                json.dump(vocab.vocab, f)\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaf31f-8891-4aef-a81d-7dad4fc55767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
