{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e794d9-8cf7-4653-a16f-bb0fc4868e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 04:48:54.619633: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-23 04:48:54.658012: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading vocabulary...\n",
      "Loading Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA Adapters from xlsr_lora_gibberish_best...\n",
      "Reading input CSV: geo/test.csv\n",
      "Starting inference on 1000 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/1000 [00:00<?, ?file/s]/m/scicomp/software/scibuilder-mamba/aalto-ubuntu2204/prod/software/scicomp-python-env/2025.2/b4b5f8e/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/m/scicomp/software/scibuilder-mamba/aalto-ubuntu2204/prod/software/scicomp-python-env/2025.2/b4b5f8e/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
      "/tmp/ipykernel_1557917/4024265065.py:134: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'mie rikinemthiztru eztez por zeaj zoldatoj kiel bona batru' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'transcript'] = transcription\n",
      "  0%|                                                                                | 1/1000 [00:00<02:54,  5.72file/s]/m/scicomp/software/scibuilder-mamba/aalto-ubuntu2204/prod/software/scicomp-python-env/2025.2/b4b5f8e/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/m/scicomp/software/scibuilder-mamba/aalto-ubuntu2204/prod/software/scicomp-python-env/2025.2/b4b5f8e/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:32<00:00, 30.74file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to test_predictions_0.2982.csv...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"base_model\": \"facebook/wav2vec2-xls-r-1b\",\n",
    "    \"checkpoint_path\": \"xlsr_lora_1b_best\", # Your saved LoRA folder\n",
    "    \"vocab_path\": \"vocab_1b.json\",                    # Your saved Vocab file\n",
    "    \"input_csv\": \"geo/test.csv\",                   # Input file\n",
    "    \"output_csv\": \"test_predictions1b_0.2982.csv\",      # Output file\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "def load_resources():\n",
    "    print(f\"Using device: {CONFIG['device']}\")\n",
    "    \n",
    "    # 1. Load Vocabulary\n",
    "    if not os.path.exists(CONFIG[\"vocab_path\"]):\n",
    "        raise FileNotFoundError(f\"Vocab file not found at {CONFIG['vocab_path']}. Did you save it during training?\")\n",
    "        \n",
    "    print(\"Loading vocabulary...\")\n",
    "    with open(CONFIG[\"vocab_path\"], \"r\") as f:\n",
    "        vocab_dict = json.load(f)\n",
    "    # Create inverse vocab (ID -> Char) for decoding\n",
    "    inv_vocab = {v: k for k, v in vocab_dict.items()}\n",
    "    \n",
    "    # 2. Load Base Model\n",
    "    print(\"Loading Base Model...\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        CONFIG[\"base_model\"], \n",
    "        vocab_size=len(vocab_dict),\n",
    "        pad_token_id=vocab_dict.get(\"<pad>\", 0),\n",
    "        ignore_mismatched_sizes=True,\n",
    "        token=False # Force anonymous to avoid 401 errors\n",
    "    )\n",
    "    \n",
    "    # 3. Load LoRA Adapters\n",
    "    print(f\"Loading LoRA Adapters from {CONFIG['checkpoint_path']}...\")\n",
    "    if not os.path.exists(CONFIG[\"checkpoint_path\"]):\n",
    "        raise FileNotFoundError(f\"Checkpoint folder not found at {CONFIG['checkpoint_path']}\")\n",
    "        \n",
    "    model = PeftModel.from_pretrained(model, CONFIG[\"checkpoint_path\"])\n",
    "    model.to(CONFIG[\"device\"])\n",
    "    model.eval()\n",
    "    \n",
    "    # 4. Load Processor\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG[\"base_model\"], token=False)\n",
    "    \n",
    "    return model, processor, inv_vocab\n",
    "\n",
    "def transcribe_single_audio(audio_path, model, processor, inv_vocab):\n",
    "    \"\"\"\n",
    "    Reads audio, resamples, and returns string transcription.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(audio_path):\n",
    "        return \"[ERROR: FILE NOT FOUND]\"\n",
    "\n",
    "    try:\n",
    "        # Load Audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample to 16k if needed\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "        \n",
    "        # Convert Stereo to Mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Preprocess\n",
    "        input_values = processor(\n",
    "            waveform.squeeze().numpy(), \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_values\n",
    "        \n",
    "        input_values = input_values.to(CONFIG[\"device\"])\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        \n",
    "        # Greedy Decode (Argmax)\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        pred_ids = pred_ids[0].cpu().tolist()\n",
    "        \n",
    "        # Decode to String (CTC Logic)\n",
    "        pred_str = []\n",
    "        prev_token = -1\n",
    "        for token in pred_ids:\n",
    "            if token != prev_token and token != 0: # 0 is <pad>\n",
    "                char = inv_vocab.get(token, \"\")\n",
    "                # Filter out special tokens just in case\n",
    "                if char not in [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\"]:\n",
    "                    pred_str.append(char)\n",
    "            prev_token = token\n",
    "            \n",
    "        # Join characters and replace pipe with space\n",
    "        final_text = \"\".join(pred_str).replace(\"|\", \" \")\n",
    "        return final_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR: {str(e)}]\"\n",
    "\n",
    "def main():\n",
    "    # 1. Setup Resources\n",
    "    model, processor, inv_vocab = load_resources()\n",
    "    \n",
    "    # 2. Load CSV\n",
    "    print(f\"Reading input CSV: {CONFIG['input_csv']}\")\n",
    "    df = pd.read_csv(CONFIG[\"input_csv\"], delimiter=\",\")\n",
    "    \n",
    "    # Ensure 'transcript' column exists\n",
    "    if 'transcript' not in df.columns:\n",
    "        df['transcript'] = \"\"\n",
    "    \n",
    "    # 3. Iterate and Predict\n",
    "    print(f\"Starting inference on {len(df)} files...\")\n",
    "    \n",
    "    # We iterate through the DataFrame using tqdm for a progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), unit=\"file\"):\n",
    "        file_path = row['file']\n",
    "        \n",
    "        # Generate transcription\n",
    "        transcription = transcribe_single_audio(file_path, model, processor, inv_vocab)\n",
    "        \n",
    "        # Save to DataFrame in memory\n",
    "        df.at[index, 'transcript'] = transcription\n",
    "\n",
    "    # 4. Save Results\n",
    "    print(f\"Saving results to {CONFIG['output_csv']}...\")\n",
    "    df.to_csv(CONFIG['output_csv'], index=False)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a68e8-f886-45e1-a9d6-c2ea5e943eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
