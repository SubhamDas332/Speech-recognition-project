{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb2075-8f36-422b-9328-0cf6bb23d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 05:20:07.547151: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-23 05:20:07.585439: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing Processor...\n",
      "Loaded 6000 valid samples from geo/train.csv (Filtered out 0)\n",
      "Loaded 1000 valid samples from geo/dev.csv (Filtered out 0)\n",
      "Loading Base Model: facebook/wav2vec2-xls-r-1b...\n",
      "\n",
      "[INFO] Found existing checkpoint at xlsr_lora_1b_best. RESUMING TRAINING...\n",
      "trainable params: 71,082,789 || all params: 1,033,627,594 || trainable%: 6.8770\n",
      "\n",
      "Starting Training Loop...\n",
      "Checking baseline performance of loaded model... \n",
      "Baseline restored -> WER: 1.0000 | CER: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|█████████████████████████████████████████| 2000/2000 [13:40<00:00,  2.44it/s, Loss=2.9223, LR=2.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 3.0872 | WER: 1.0000 | CER: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|█████████████████████████████████████████| 2000/2000 [13:38<00:00,  2.44it/s, Loss=1.0335, LR=4.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 2.5313 | WER: 0.8409 | CER: 0.2856\n",
      "New Best Model Saved! (WER: 0.8409)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|█████████████████████████████████████████| 2000/2000 [13:36<00:00,  2.45it/s, Loss=0.4988, LR=6.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 1.0273 | WER: 0.6217 | CER: 0.1595\n",
      "New Best Model Saved! (WER: 0.6217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|█████████████████████████████████████████| 2000/2000 [13:35<00:00,  2.45it/s, Loss=0.5830, LR=8.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.7475 | WER: 0.5483 | CER: 0.1390\n",
      "New Best Model Saved! (WER: 0.5483)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|█████████████████████████████████████████| 2000/2000 [13:41<00:00,  2.44it/s, Loss=0.6796, LR=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.5831 | WER: 0.4812 | CER: 0.1253\n",
      "New Best Model Saved! (WER: 0.4812)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|█████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.5366, LR=9.78e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.4887 | WER: 0.4674 | CER: 0.1230\n",
      "New Best Model Saved! (WER: 0.4674)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|█████████████████████████████████████████| 2000/2000 [13:36<00:00,  2.45it/s, Loss=0.3604, LR=9.56e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 0.4270 | WER: 0.4310 | CER: 0.1114\n",
      "New Best Model Saved! (WER: 0.4310)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|█████████████████████████████████████████| 2000/2000 [13:39<00:00,  2.44it/s, Loss=0.3222, LR=9.33e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.3727 | WER: 0.4078 | CER: 0.1060\n",
      "New Best Model Saved! (WER: 0.4078)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|█████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.8841, LR=9.11e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.3487 | WER: 0.3790 | CER: 0.0987\n",
      "New Best Model Saved! (WER: 0.3790)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|████████████████████████████████████████| 2000/2000 [13:32<00:00,  2.46it/s, Loss=0.2533, LR=8.89e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 0.3199 | WER: 0.3605 | CER: 0.0933\n",
      "New Best Model Saved! (WER: 0.3605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.2373, LR=8.67e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss: 0.3013 | WER: 0.3432 | CER: 0.0870\n",
      "New Best Model Saved! (WER: 0.3432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|████████████████████████████████████████| 2000/2000 [13:34<00:00,  2.45it/s, Loss=0.3431, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss: 0.2869 | WER: 0.3351 | CER: 0.0857\n",
      "New Best Model Saved! (WER: 0.3351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|████████████████████████████████████████| 2000/2000 [13:36<00:00,  2.45it/s, Loss=0.1637, LR=8.22e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss: 0.2747 | WER: 0.3182 | CER: 0.0802\n",
      "New Best Model Saved! (WER: 0.3182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|████████████████████████████████████████| 2000/2000 [13:36<00:00,  2.45it/s, Loss=0.1857, LR=8.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss: 0.2566 | WER: 0.3092 | CER: 0.0785\n",
      "New Best Model Saved! (WER: 0.3092)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|████████████████████████████████████████| 2000/2000 [13:33<00:00,  2.46it/s, Loss=0.2771, LR=7.78e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Loss: 0.2468 | WER: 0.3021 | CER: 0.0758\n",
      "New Best Model Saved! (WER: 0.3021)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.2112, LR=7.56e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Loss: 0.2298 | WER: 0.3042 | CER: 0.0776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|████████████████████████████████████████| 2000/2000 [13:34<00:00,  2.46it/s, Loss=0.2446, LR=7.33e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Loss: 0.2280 | WER: 0.2942 | CER: 0.0741\n",
      "New Best Model Saved! (WER: 0.2942)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.2886, LR=7.11e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Loss: 0.2223 | WER: 0.2940 | CER: 0.0735\n",
      "New Best Model Saved! (WER: 0.2940)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.0551, LR=6.89e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Loss: 0.2139 | WER: 0.2818 | CER: 0.0721\n",
      "New Best Model Saved! (WER: 0.2818)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|████████████████████████████████████████| 2000/2000 [13:38<00:00,  2.44it/s, Loss=0.2631, LR=6.67e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Loss: 0.2070 | WER: 0.2761 | CER: 0.0700\n",
      "New Best Model Saved! (WER: 0.2761)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.0884, LR=6.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Loss: 0.1982 | WER: 0.2839 | CER: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|████████████████████████████████████████| 2000/2000 [13:37<00:00,  2.45it/s, Loss=0.4031, LR=6.22e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Loss: 0.1878 | WER: 0.2665 | CER: 0.0665\n",
      "New Best Model Saved! (WER: 0.2665)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|████████████████████████████████████████| 2000/2000 [13:36<00:00,  2.45it/s, Loss=0.2669, LR=6.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Loss: 0.1881 | WER: 0.2651 | CER: 0.0670\n",
      "New Best Model Saved! (WER: 0.2651)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|████████████████████████████████████████| 2000/2000 [13:36<00:00,  2.45it/s, Loss=0.2518, LR=5.78e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Loss: 0.1876 | WER: 0.2484 | CER: 0.0617\n",
      "New Best Model Saved! (WER: 0.2484)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|████████████████████████████████████████| 2000/2000 [13:35<00:00,  2.45it/s, Loss=0.2881, LR=5.56e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Loss: 0.1716 | WER: 0.2556 | CER: 0.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|████████████████████████████████████████| 2000/2000 [13:41<00:00,  2.43it/s, Loss=0.0593, LR=5.33e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Loss: 0.1694 | WER: 0.2484 | CER: 0.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|████████████████████████████████████████| 2000/2000 [13:38<00:00,  2.44it/s, Loss=0.3493, LR=5.11e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Loss: 0.1682 | WER: 0.2453 | CER: 0.0605\n",
      "New Best Model Saved! (WER: 0.2453)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50:   4%|█▌                                        | 77/2000 [00:31<14:21,  2.23it/s, Loss=0.0723, LR=5.10e-05]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer\n",
    "import gc\n",
    "import types\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Hugging Face Imports\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from huggingface_hub import login\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.getLogger(\"torchaudio\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"train_csv\": \"geo/train.csv\",\n",
    "    \"val_csv\": \"geo/dev.csv\",\n",
    "    \"hf_token\": None,  \n",
    "    \n",
    "    # Hyperparameters\n",
    "    \"mask_time_prob\": 0.05,\n",
    "    \"mask_time_length\": 10,\n",
    "    \"mask_feature_prob\": 0.05,\n",
    "    \"mask_feature_length\": 10,\n",
    "    \n",
    "    \"lora_rank\": 64,             \n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    \"batch_size\": 3,           \n",
    "    \"grad_accum_steps\": 8,     \n",
    "    \"max_audio_len\": 160000,   \n",
    "    \"learning_rate\": 1e-4,     \n",
    "    \"num_epochs\": 50,            \n",
    "    \n",
    "    \"base_model\": \"facebook/wav2vec2-xls-r-1b\",\n",
    "    \n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"intermediate_dense\", \"output_dense\"],\n",
    "    \"checkpoint_path\": \"xlsr_lora_1b_best\",\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if CONFIG[\"hf_token\"]:\n",
    "    login(token=CONFIG[\"hf_token\"])\n",
    "\n",
    "# --- Vocabulary Builder ---\n",
    "class Vocabulary:\n",
    "    def __init__(self, csv_paths):\n",
    "        self.vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3, \"|\": 4}\n",
    "        self.idx = 5\n",
    "        self._build_vocab(csv_paths)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def _build_vocab(self, paths):\n",
    "        chars = set()\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path): continue\n",
    "            df = pd.read_csv(path)\n",
    "            all_text = \"\".join(df['transcript'].fillna(\"\").astype(str).tolist())\n",
    "            chars.update(list(all_text))\n",
    "        \n",
    "        for c in sorted(chars):\n",
    "            if c not in self.vocab:\n",
    "                self.vocab[c] = self.idx\n",
    "                self.idx += 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = str(text).replace(\" \", \"|\")\n",
    "        return [self.vocab.get(c, self.vocab[\"<unk>\"]) for c in text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        res = []\n",
    "        for t in tokens:\n",
    "            if t == 0: continue \n",
    "            char = self.inv_vocab.get(t, \"\")\n",
    "            res.append(char)\n",
    "        return \"\".join(res).replace(\"|\", \" \").replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "# --- Dataset ---\n",
    "class EsperantoDataset(Dataset):\n",
    "    def __init__(self, csv_path, vocab, processor, max_len=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        initial_len = len(self.df)\n",
    "        self.df = self.df[self.df['file'].apply(os.path.exists)].reset_index(drop=True)\n",
    "        print(f\"Loaded {len(self.df)} valid samples from {csv_path} (Filtered out {initial_len - len(self.df)})\")\n",
    "        self.vocab = vocab\n",
    "        self.processor = processor \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['file']\n",
    "        transcript = self.df.iloc[idx]['transcript']\n",
    "        if pd.isna(transcript): transcript = \"\"\n",
    "        transcript = str(transcript)\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            waveform = waveform.squeeze()\n",
    "            if self.max_len and waveform.size(0) > self.max_len:\n",
    "                waveform = waveform[:self.max_len]\n",
    "\n",
    "            features = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "            labels = torch.tensor(self.vocab.encode(transcript), dtype=torch.long)\n",
    "            return features, labels\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch: return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "    features, labels = zip(*batch)\n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0.0)\n",
    "    attention_mask = (features_padded != 0).long() \n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return features_padded, attention_mask, labels_padded\n",
    "\n",
    "# --- Model Loading Logic (Modified for Resume) ---\n",
    "def get_lora_model(vocab_size):\n",
    "    print(f\"Loading Base Model: {CONFIG['base_model']}...\")\n",
    "    use_auth_token = CONFIG[\"hf_token\"] if CONFIG[\"hf_token\"] else False\n",
    "\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        CONFIG[\"base_model\"], \n",
    "        ctc_loss_reduction=\"mean\", \n",
    "        pad_token_id=0,\n",
    "        vocab_size=vocab_size,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        token=use_auth_token,\n",
    "        mask_time_prob=CONFIG[\"mask_time_prob\"],\n",
    "        mask_time_length=CONFIG[\"mask_time_length\"],\n",
    "        mask_feature_prob=CONFIG[\"mask_feature_prob\"],\n",
    "        mask_feature_length=CONFIG[\"mask_feature_length\"],\n",
    "    )\n",
    "    \n",
    "    # Gradient Checkpointing Hook\n",
    "    if not hasattr(model, \"enable_input_require_grads\"):\n",
    "        def enable_input_require_grads(self):\n",
    "            def make_inputs_require_grads(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            self.wav2vec2.feature_projection.register_forward_hook(make_inputs_require_grads)\n",
    "        model.enable_input_require_grads = types.MethodType(enable_input_require_grads, model)\n",
    "\n",
    "    # Check for existing checkpoint to RESUME\n",
    "    adapter_path = os.path.join(CONFIG[\"checkpoint_path\"], \"adapter_model.safetensors\")\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        print(f\"\\n[INFO] Found existing checkpoint at {CONFIG['checkpoint_path']}. RESUMING TRAINING...\")\n",
    "        # is_trainable=True is REQUIRED to continue training\n",
    "        model = PeftModel.from_pretrained(model, CONFIG[\"checkpoint_path\"], is_trainable=True)\n",
    "    else:\n",
    "        print(f\"\\n[INFO] No checkpoint found. Initializing FRESH LoRA model...\")\n",
    "        modules_to_save = [\"lm_head\", \"layer_norm\"] \n",
    "        peft_config = LoraConfig(\n",
    "            inference_mode=False,\n",
    "            r=CONFIG[\"lora_rank\"],\n",
    "            lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "            lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "            target_modules=CONFIG[\"target_modules\"],\n",
    "            modules_to_save=modules_to_save\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "# --- Helper: Evaluation Function ---\n",
    "def evaluate(model, dataloader, vocab):\n",
    "    model.eval()\n",
    "    refs, preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, mask, labels in dataloader:\n",
    "            if features.size(0) == 0: continue\n",
    "            features = features.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(features, attention_mask=mask)\n",
    "            \n",
    "            logits = outputs.logits.float().cpu().numpy()\n",
    "            pred_ids = logits.argmax(axis=-1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                label_idx = labels[i][labels[i] != -100].cpu().tolist()\n",
    "                refs.append(vocab.decode(label_idx))\n",
    "                \n",
    "                pred_str_raw = []\n",
    "                prev_token = -1\n",
    "                for token in pred_ids[i]:\n",
    "                    if token != prev_token and token != 0: \n",
    "                        pred_str_raw.append(token)\n",
    "                    prev_token = token\n",
    "                preds.append(vocab.decode(pred_str_raw))\n",
    "\n",
    "    epoch_wer = wer(refs, preds) if len(refs) > 0 else 1.0\n",
    "    epoch_cer = cer(refs, preds) if len(refs) > 0 else 1.0\n",
    "    return epoch_wer, epoch_cer\n",
    "\n",
    "# --- Main ---\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    vocab = Vocabulary([CONFIG[\"train_csv\"], CONFIG[\"val_csv\"]])\n",
    "    use_auth_token = CONFIG[\"hf_token\"] if CONFIG[\"hf_token\"] else False\n",
    "\n",
    "    print(\"Initializing Processor...\")\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG[\"base_model\"], token=use_auth_token)\n",
    "    \n",
    "    train_dataset = EsperantoDataset(CONFIG[\"train_csv\"], vocab, processor, max_len=CONFIG[\"max_audio_len\"])\n",
    "    val_dataset = EsperantoDataset(CONFIG[\"val_csv\"], vocab, processor) \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn, num_workers=1)\n",
    "\n",
    "    # LOAD MODEL (Handles Resume)\n",
    "    model = get_lora_model(len(vocab.vocab))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    num_training_steps = len(train_loader) * CONFIG[\"num_epochs\"] // CONFIG[\"grad_accum_steps\"]\n",
    "    num_warmup_steps = int(0.1 * num_training_steps) \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    print(f\"\\nStarting Training Loop...\")\n",
    "    \n",
    "    # --- CRITICAL: Establish Baseline ---\n",
    "    # If we resumed, we must know the current WER. If we don't check,\n",
    "    # we might overwrite a good model (WER 0.20) with a bad first epoch (WER 0.25)\n",
    "    # because best_wer would start at infinity.\n",
    "    best_wer = float('inf')\n",
    "    \n",
    "    if os.path.exists(os.path.join(CONFIG[\"checkpoint_path\"], \"adapter_model.safetensors\")):\n",
    "        print(\"Checking baseline performance of loaded model... \")\n",
    "        baseline_wer, baseline_cer = evaluate(model, val_loader, vocab)\n",
    "        best_wer = baseline_wer\n",
    "        print(f\"Baseline restored -> WER: {baseline_wer:.4f} | CER: {baseline_cer:.4f}\")\n",
    "    \n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        for step, (features, mask, labels) in enumerate(pbar):\n",
    "            if features.size(0) == 0: continue \n",
    "\n",
    "            features = features.to(device)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(features, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss / CONFIG[\"grad_accum_steps\"]\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % CONFIG[\"grad_accum_steps\"] == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * CONFIG[\"grad_accum_steps\"]\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item()*CONFIG['grad_accum_steps']:.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        print(f\"Evaluating Epoch {epoch+1}...\")\n",
    "        epoch_wer, epoch_cer = evaluate(model, val_loader, vocab)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_train_loss:.4f} | WER: {epoch_wer:.4f} | CER: {epoch_cer:.4f}\")\n",
    "        \n",
    "        if epoch_wer < best_wer:\n",
    "            best_wer = epoch_wer\n",
    "            model.save_pretrained(CONFIG[\"checkpoint_path\"])\n",
    "            print(f\"New Best Model Saved! (WER: {best_wer:.4f})\")\n",
    "            \n",
    "            # Also save vocab whenever we save a new best model\n",
    "            with open(\"vocab_1b.json\", \"w\") as f:\n",
    "                json.dump(vocab.vocab, f)\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
