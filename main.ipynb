{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6da01c6-af59-44ba-8f89-ddd3dc454702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 04:20:04.150263: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-23 04:20:04.189586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing Processor...\n",
      "Loaded 6000 valid samples from geo/train.csv (Filtered out 0)\n",
      "Loaded 1000 valid samples from geo/dev.csv (Filtered out 0)\n",
      "Loading Base Model: facebook/wav2vec2-large-xlsr-53...\n",
      "\n",
      "[INFO] Found existing checkpoint at xlsr_lora_gibberish_best. RESUMING TRAINING...\n",
      "trainable params: 28,458,021 || all params: 343,934,666 || trainable%: 8.2743\n",
      "\n",
      "Starting Training Loop...\n",
      "Checking baseline performance of loaded model... \n",
      "Baseline restored -> WER: 0.2982 | CER: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|█████████████████████████████████████████| 1000/1000 [05:40<00:00,  2.93it/s, Loss=0.2482, LR=1.66e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.2166 | WER: 0.2994 | CER: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|█████████████████████████████████████████| 1000/1000 [05:40<00:00,  2.93it/s, Loss=0.1789, LR=3.32e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.2143 | WER: 0.2998 | CER: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|█████████████████████████████████████████| 1000/1000 [05:41<00:00,  2.93it/s, Loss=0.1928, LR=4.97e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.2116 | WER: 0.3000 | CER: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|█████████████████████████████████████████| 1000/1000 [05:42<00:00,  2.92it/s, Loss=0.1534, LR=4.98e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.2137 | WER: 0.3002 | CER: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30:  55%|███████████████████████▏                  | 552/1000 [03:08<02:33,  2.93it/s, Loss=0.1528, LR=4.96e-06]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 322\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 299\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    297\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 299\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_accum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    300\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_accum_steps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    302\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer\n",
    "import gc\n",
    "import types\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Hugging Face Imports\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from huggingface_hub import login\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.getLogger(\"torchaudio\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"train_csv\": \"geo/train.csv\",\n",
    "    \"val_csv\": \"geo/dev.csv\",\n",
    "    \"hf_token\": None,  \n",
    "    \n",
    "    # Hyperparameters\n",
    "    \"mask_time_prob\": 0.025,\n",
    "    \"mask_time_length\": 10,\n",
    "    \"mask_feature_prob\": 0.025,\n",
    "    \"mask_feature_length\": 10,\n",
    "    \n",
    "    \"lora_rank\": 64,             \n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    \"batch_size\": 6,           \n",
    "    \"grad_accum_steps\": 16,     \n",
    "    \"max_audio_len\": 160000,   \n",
    "    \"learning_rate\": 5e-6,     \n",
    "    \"num_epochs\": 30,            \n",
    "    \n",
    "    \"base_model\": \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    \n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"intermediate_dense\", \"output_dense\"],\n",
    "    \"checkpoint_path\": \"xlsr_lora_gibberish_best\",\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if CONFIG[\"hf_token\"]:\n",
    "    login(token=CONFIG[\"hf_token\"])\n",
    "\n",
    "# --- Vocabulary Builder ---\n",
    "class Vocabulary:\n",
    "    def __init__(self, csv_paths):\n",
    "        self.vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3, \"|\": 4}\n",
    "        self.idx = 5\n",
    "        self._build_vocab(csv_paths)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def _build_vocab(self, paths):\n",
    "        chars = set()\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path): continue\n",
    "            df = pd.read_csv(path)\n",
    "            all_text = \"\".join(df['transcript'].fillna(\"\").astype(str).tolist())\n",
    "            chars.update(list(all_text))\n",
    "        \n",
    "        for c in sorted(chars):\n",
    "            if c not in self.vocab:\n",
    "                self.vocab[c] = self.idx\n",
    "                self.idx += 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = str(text).replace(\" \", \"|\")\n",
    "        return [self.vocab.get(c, self.vocab[\"<unk>\"]) for c in text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        res = []\n",
    "        for t in tokens:\n",
    "            if t == 0: continue \n",
    "            char = self.inv_vocab.get(t, \"\")\n",
    "            res.append(char)\n",
    "        return \"\".join(res).replace(\"|\", \" \").replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "# --- Dataset ---\n",
    "class EsperantoDataset(Dataset):\n",
    "    def __init__(self, csv_path, vocab, processor, max_len=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        initial_len = len(self.df)\n",
    "        self.df = self.df[self.df['file'].apply(os.path.exists)].reset_index(drop=True)\n",
    "        print(f\"Loaded {len(self.df)} valid samples from {csv_path} (Filtered out {initial_len - len(self.df)})\")\n",
    "        self.vocab = vocab\n",
    "        self.processor = processor \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['file']\n",
    "        transcript = self.df.iloc[idx]['transcript']\n",
    "        if pd.isna(transcript): transcript = \"\"\n",
    "        transcript = str(transcript)\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            waveform = waveform.squeeze()\n",
    "            if self.max_len and waveform.size(0) > self.max_len:\n",
    "                waveform = waveform[:self.max_len]\n",
    "\n",
    "            features = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "            labels = torch.tensor(self.vocab.encode(transcript), dtype=torch.long)\n",
    "            return features, labels\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch: return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "    features, labels = zip(*batch)\n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0.0)\n",
    "    attention_mask = (features_padded != 0).long() \n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return features_padded, attention_mask, labels_padded\n",
    "\n",
    "# --- Model Loading Logic (Modified for Resume) ---\n",
    "def get_lora_model(vocab_size):\n",
    "    print(f\"Loading Base Model: {CONFIG['base_model']}...\")\n",
    "    use_auth_token = CONFIG[\"hf_token\"] if CONFIG[\"hf_token\"] else False\n",
    "\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        CONFIG[\"base_model\"], \n",
    "        ctc_loss_reduction=\"mean\", \n",
    "        pad_token_id=0,\n",
    "        vocab_size=vocab_size,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        token=use_auth_token,\n",
    "        mask_time_prob=CONFIG[\"mask_time_prob\"],\n",
    "        mask_time_length=CONFIG[\"mask_time_length\"],\n",
    "        mask_feature_prob=CONFIG[\"mask_feature_prob\"],\n",
    "        mask_feature_length=CONFIG[\"mask_feature_length\"],\n",
    "    )\n",
    "    \n",
    "    # Gradient Checkpointing Hook\n",
    "    if not hasattr(model, \"enable_input_require_grads\"):\n",
    "        def enable_input_require_grads(self):\n",
    "            def make_inputs_require_grads(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            self.wav2vec2.feature_projection.register_forward_hook(make_inputs_require_grads)\n",
    "        model.enable_input_require_grads = types.MethodType(enable_input_require_grads, model)\n",
    "\n",
    "    # Check for existing checkpoint to RESUME\n",
    "    adapter_path = os.path.join(CONFIG[\"checkpoint_path\"], \"adapter_model.safetensors\")\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        print(f\"\\n[INFO] Found existing checkpoint at {CONFIG['checkpoint_path']}. RESUMING TRAINING...\")\n",
    "        # is_trainable=True is REQUIRED to continue training\n",
    "        model = PeftModel.from_pretrained(model, CONFIG[\"checkpoint_path\"], is_trainable=True)\n",
    "    else:\n",
    "        print(f\"\\n[INFO] No checkpoint found. Initializing FRESH LoRA model...\")\n",
    "        modules_to_save = [\"lm_head\", \"layer_norm\"] \n",
    "        peft_config = LoraConfig(\n",
    "            inference_mode=False,\n",
    "            r=CONFIG[\"lora_rank\"],\n",
    "            lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "            lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "            target_modules=CONFIG[\"target_modules\"],\n",
    "            modules_to_save=modules_to_save\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "# --- Helper: Evaluation Function ---\n",
    "def evaluate(model, dataloader, vocab):\n",
    "    model.eval()\n",
    "    refs, preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, mask, labels in dataloader:\n",
    "            if features.size(0) == 0: continue\n",
    "            features = features.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(features, attention_mask=mask)\n",
    "            \n",
    "            logits = outputs.logits.float().cpu().numpy()\n",
    "            pred_ids = logits.argmax(axis=-1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                label_idx = labels[i][labels[i] != -100].cpu().tolist()\n",
    "                refs.append(vocab.decode(label_idx))\n",
    "                \n",
    "                pred_str_raw = []\n",
    "                prev_token = -1\n",
    "                for token in pred_ids[i]:\n",
    "                    if token != prev_token and token != 0: \n",
    "                        pred_str_raw.append(token)\n",
    "                    prev_token = token\n",
    "                preds.append(vocab.decode(pred_str_raw))\n",
    "\n",
    "    epoch_wer = wer(refs, preds) if len(refs) > 0 else 1.0\n",
    "    epoch_cer = cer(refs, preds) if len(refs) > 0 else 1.0\n",
    "    return epoch_wer, epoch_cer\n",
    "\n",
    "# --- Main ---\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    vocab = Vocabulary([CONFIG[\"train_csv\"], CONFIG[\"val_csv\"]])\n",
    "    use_auth_token = CONFIG[\"hf_token\"] if CONFIG[\"hf_token\"] else False\n",
    "\n",
    "    print(\"Initializing Processor...\")\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG[\"base_model\"], token=use_auth_token)\n",
    "    \n",
    "    train_dataset = EsperantoDataset(CONFIG[\"train_csv\"], vocab, processor, max_len=CONFIG[\"max_audio_len\"])\n",
    "    val_dataset = EsperantoDataset(CONFIG[\"val_csv\"], vocab, processor) \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn, num_workers=1)\n",
    "\n",
    "    # LOAD MODEL (Handles Resume)\n",
    "    model = get_lora_model(len(vocab.vocab))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    num_training_steps = len(train_loader) * CONFIG[\"num_epochs\"] // CONFIG[\"grad_accum_steps\"]\n",
    "    num_warmup_steps = int(0.1 * num_training_steps) \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    print(f\"\\nStarting Training Loop...\")\n",
    "    \n",
    "    # --- CRITICAL: Establish Baseline ---\n",
    "    # If we resumed, we must know the current WER. If we don't check,\n",
    "    # we might overwrite a good model (WER 0.20) with a bad first epoch (WER 0.25)\n",
    "    # because best_wer would start at infinity.\n",
    "    best_wer = float('inf')\n",
    "    \n",
    "    if os.path.exists(os.path.join(CONFIG[\"checkpoint_path\"], \"adapter_model.safetensors\")):\n",
    "        print(\"Checking baseline performance of loaded model... \")\n",
    "        baseline_wer, baseline_cer = evaluate(model, val_loader, vocab)\n",
    "        best_wer = baseline_wer\n",
    "        print(f\"Baseline restored -> WER: {baseline_wer:.4f} | CER: {baseline_cer:.4f}\")\n",
    "    \n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        for step, (features, mask, labels) in enumerate(pbar):\n",
    "            if features.size(0) == 0: continue \n",
    "\n",
    "            features = features.to(device)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(features, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss / CONFIG[\"grad_accum_steps\"]\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % CONFIG[\"grad_accum_steps\"] == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * CONFIG[\"grad_accum_steps\"]\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item()*CONFIG['grad_accum_steps']:.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        print(f\"Evaluating Epoch {epoch+1}...\")\n",
    "        epoch_wer, epoch_cer = evaluate(model, val_loader, vocab)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_train_loss:.4f} | WER: {epoch_wer:.4f} | CER: {epoch_cer:.4f}\")\n",
    "        \n",
    "        if epoch_wer < best_wer:\n",
    "            best_wer = epoch_wer\n",
    "            model.save_pretrained(CONFIG[\"checkpoint_path\"])\n",
    "            print(f\"New Best Model Saved! (WER: {best_wer:.4f})\")\n",
    "            \n",
    "            # Also save vocab whenever we save a new best model\n",
    "            with open(\"vocab.json\", \"w\") as f:\n",
    "                json.dump(vocab.vocab, f)\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
